# intro-to-HPC-course-2020
Introduction to HPC/working on the LRZ linux cluster

Some material adapted from: HPC Carpentry - https://hpc-carpentry.github.io/hpc-intro/

This course is an introduction to working in an HPC environment and will cover why we use this and the basics of logging in and submitting jobs.

## What is a cluster? 

Very simply, a cluster is a group of computers connected via a very fast network connection and using special software which allows them to work as one. Each individual computers that make up a cluster is termed a node. 

On a typical cluster set-up, there are different types of nodes for performing different types of tasks. For example, when logging in to a cluster from your local machine you will likely on the login node (also called the head node or submit node). This node serves as an access point to the cluster. These nodes are used for uploading and downloading files, setting up software, and running quick tests. __They should never be used for doing actual work__. (You will likely be banned from using the cluster by your system administrator if you try to run a large job on the login node)

The heavy lifting on a cluster is done by the compute (or worker) nodes. There are lots of different types and configurations but generally these nodes have large amounts of memory and powerful CPUs and are set up to perform long running tasks. Here is the configuration of the nodes on the LRZ linux cluster.


https://doku.lrz.de/display/PUBLIC/Job+Processing+on+the+Linux-Cluster


You will not usually work on the compute nodes directly (except in the case of interactive mode which I will explain later). Generally, interaction with these nodes is through  a job scheduler (the scheduler used on the Linux cluster is called SLURM). This software uses an algorithm to optimise the scheduling of jobs and assignment of resources, while also allowing resource monitoring. 

### Difference between cloud computing and a cluster 

You may have heard the term cloud computing. But what's the difference between cloud computing and HPC? At it's most basic, the difference is that HPC represents a physical system where the resources are fixed (i.e. the CoolMUC2 cluster we will be using today) while cloud computing refers more to scalable, on-demand computing, for example AWS or Google cloud. That is, the resources are not necessarily fixed and can be increased or decreased depending on your requirements. Of course, there are other differences too but for now this is the main difference that matters for us.  


### Why use a cluster?

For analysis of most of the biological data we generate in this lab at the moment we don't really need a cluster. However, as sequencing gets cheaper and cheaper and more commonplace it will probably become a larger part of the data we produce. Right now, most of us have at least one set of sequencing data, whether that is 16S, Shotgun or RNAseq. 

The majority of 16S analysis can be performd without utilising HPC but that does not mean that it isn't useful for this purpose (I will provide an example of how it can be useful at the end of this course).  Most of the laptops or PCs we have will have a maximum of 16GB RAM and a 3 or 4GhZ quad-core processor. This is a lot for everyday use but simply isn't enough to work with huge datasets generated by NGS or run some tools for analysis. As an example, I often run PICRUSt2, which requires an absolute minimum of 16GB, around the upper limits of our personal machines. 

Shotgun and RNAseq in most cases, requires access to very powerful hardware as datasets can be tens to hundreds of GBs in size. Of course you can always outsource this but it's good to at least have an understanding of the process and if you are able to perform these analyses yourself it takes less time to get the data and figures. 


## Practical 

## Refresher: Working on a Unix system 

Hopefully you have taken (or at least had a quick look at) the previous course I did: https://github.com/adamsorbie/unix_shell_course-2020-02-14

If not we will do a very short refresher on working on a Unix system. 

### Finding your way around 

Print the working directory 

```
$ pwd
```
Example output on my machine:

```
/home/adamsorbie
``` 

List files and folders

```
$ ls
``` 

Example output:

```
bin/ analyses/ Documents/ 
```
Creating new directories (this might be necessary if you have a new installation of WSL) 

```
$ mkdir test
```
Check it worked: 

```
$ ls 
``` 

Expected output: 

```
test/
``` 

Moving around the file system 

```
$ cd bin/ 
```

Test with pwd 
```
$ pwd
``` 
Output on my machine:

``` 
/home/adamsorbie/bin
```
Play around with these examples and those in the linked course for 5 minutes or so until you feel comfortable with these concepts. 

## Access and login 

To access the Linux cluster we will be using ssh (Note: those of you who do not have an account with LRZ will be unable to do this themselves but I will explain how to do this on my machine). 

## First time set-up

These are the nodes you are likely to use to login. 

| Command  | Login Node |
|---------------|-------|
| ssh -Y lxlogin1.lrz.de -l <user> | Haswell (CoolMUC-2) login node |
| ssh -Y lxlogin2.lrz.de -l <user> | Haswell (CoolMUC-2) login node |
| ssh -Y lxlogin3.lrz.de -l <user> | Haswell (CoolMUC-2) login node |
| ssh -Y lxlogin4.lrz.de -l <user> | Haswell (CoolMUC-2) login node |

Important info from LRZ!!!!! 

"The login nodes are meant for preparing your jobs, developing your programs, and as a gateway for copying data from your own computer to the cluster and back again. Since this resource is shared among many users, LRZ requires that you do not start any long-running or memory-hogging programs on these nodes; production runs should use batch jobs that are submitted to the SLURM scheduler. Our SLURM configuration also supports semi-interactive testing. Violation of the usage restrictions on the login nodes may lead to your account being blocked from further access to the cluster, apart from your processes being forcibly removed by LRZ administrative staff!"

Additionally, the following steps are super important to make sure your connection is secure. Multiple HPC centres across Europe were hacked recently, including the LRZ so it's important to follow these steps carefully. 

First of all we need to generate an SSH-key to be able to access the LRZ system from outside. 

Run the following command and press enter

```
ssh-keygen -t ecdsa -b 521 -a 100
```

You will be prompted to enter a passphrase , you __must__ enter something, ideally a long sentence which is memorable for you. 

If you set up an account with the LRZ then enter the following 

```
ssh-copy-id -i /home/myaccount/.ssh/id_ecdsa <user>@<targetsystem>
```

If you want you can also run the next command which will allow prompt you to enter your passphrase then subsequently allow you to execute ssh commands without reauthenticating (in the same session, i.e. if you close your terminal then restart you will need to reauthenticate). 

```
ssh-add
```

## Accessing installed software and submitting jobs 

On high-performance computing systems, it is often the case that no software is loaded by default. If we want to use a particular package, we will usually need to “load” it ourselves.

Before we start using individual software packages, it's important to understand the reasoning behind this approach. The three biggest factors are:

software incompatibilities
versioning
dependencies


Software incompatibility is a major problem for programmers and bioinformaticians alike. Sometimes the presence (or absence) of a package will break others that depend on it.  

Software versioning is another common issue. You might depend on a particular version of a package for an analysis you are doing - if the software version changes (for instance, if a package was updated), it might affect your results. This sort of thing happens all the time and can be especially problematic in R. Having access to multiple software versions allows you to prevent issues with software versioning from affecting your results.

Dependencies are where a particular software package (or even a particular version) depends on having access to another software package (or even a particular version of another software package). If the package your package depends on is not there or is the wrong version, then it may not work. 

The solution to this on HPC systems is Environment modules. These are software packages which allow you to dynamically modify the environment you are working in. We need to take a little quick detour to understand this better. When you type a command on a unix system (excluding builtins like cd and echo) the shell searches for it in your path. If it finds it then the command will run, otherwise the shell will return an error. 

To see what is currently in your path variable type the following command:

```
$ echo $PATH
```

Since hundreds or thousands of researchers are using the Linux cluster and all have different software needs, it would be really messy for everything to be installed and available at once, the output of ```$ echo $PATH``` would be pages and pages long. Environment modules manage this problem by dynamically adding packages to your path making sure you only have what you need. 

When logging to the linux cluster you start with only a few modules preloaded. For those of us who have access to the cluster, let's login now and explore this a little further. 

Type the following command to see what these are: 

```
$ module list
```

You should see something like the following:

```
Currently Loaded Modulefiles:
 1) admin/1.0     3) lrz/1.0                5) intel/19.0.5           7) intel-mpi/2019.7.217
 2) tempdir/1.0   4) spack/staging/20.1.1   6) intel-mkl/2019.5.281
```

You can also type ``` $ echo $PATH``` to check what's already in your path variable. 

To see what modules are available we can type: 

```
module avail
```

You will notice there is a few pages of packages listed, including different versions of the same software. Let's go ahead and load python. 

```
module load python
```

Note: There are multiple versions of python installed, if you do not specify a version it will just load the default. 

Now let's check our path to see if python is now included there (here we pipe the output of echo to grep and search for python to make it easier to see)

```
$ echo $PATH | grep python
```

To unload a package we can type ```module unload <module>``` Let's unload python. 

```
module unload python
```


In the last few years, some effors have been made to manage packages a little better. These efforts are mostly focused on non-HPC systems but will still be useful for you if you are working on the cluster and need to install something that isn't available in the listed modules. Since you obviously don't have administrator (Root on unix systems) privileges you generally can't install packages in the default location. To get around we can use the package manager conda. Conda is software tool which allows you create separate environments yourself and manage versions and dependencies. It's not perfect but it definitely makes managing packages a lot easier. 

We will install and create a PICRUSt2 environment here as an example. 

First we need to load the python module so we have access to conda 

```
module load python
```

Install PICRUSt2 using conda 

```
conda create -n picrust2 -c bioconda -c conda-forge picrust2=2.3.0_b python=3.6
```


## Reponsible use 


## Rstudio server 

This will likely be the most useful service for many of you. 
